\chapter{Experimental Results}\label{chap:results}

\paragraph{Implementation.}
\Forest{} is implemented in Python 3.8 on top of \textsc{Trinity}, a general-purpose synthesis framework \cite{trinity19}.
%\textsc{Trinity} was extended to support our Regular Expression \ac{DSL}, multi-tree constructs, and interaction based on distinguishing inputs.
All \ac{SMT} formulas are solved using the Z3 SMT solver, version  4.8.9 \cite{z3}. 
To find distinguishing inputs in regular expression synthesis, \Forest uses Z3's theory of regular expressions (part of the theory of strings \cite{z3str317}).
To check the enumerated regexes against the examples, we use Python's regex library \cite{PythonRe}.
%
The results presented herein were obtained using an Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz, with 64GB of RAM, running Debian GNU/Linux~10. All processes were run with a time limit of one hour.

\paragraph{Benchmarks.} To evaluate \Forest{}, we used 69 benchmarks based on real-world form-validation regular expressions. These were collected from regular expression validators in validation frameworks and from \texttt{regexlib}~\cite{regexlib}, a website where users can upload their own regexes. Among these 69 benchmarks there are different formats: national IDs, identifiers of products, date and time, vehicle registration numbers, postal codes, email and phone numbers.
For each benchmark, we randomly generated sets of strings to be used as input examples for \Forest{}. 
%All benchmarks' goal is to produce a regex validation.
All 69 benchmarks require a regular expression to validate the examples. 10 require capturing groups to extract the captures provided alongside the valid examples, and 8 require additional capture conditions to validate conditional invalid examples.
On average, each instance is composed of 13.4 valid examples (ranging from 4 to 33) and 9.0 invalid (ranging from 2 to 38). The 7 instances that target capture conditions have on average 6 conditional invalid examples (ranging from 4 to 8). 
The median number of total examples per instance is 22, while the median number of valid examples is 11, 7 for invalid, and 6 for conditional invalid.

\bigskip\noindent
The goal of this experimental evaluation is to answer the following questions:

\begin{enumerate}[label=\textbf{Q\arabic*}.]
%
\item How does \Forest{} compare against \Regel? (\autoref{sec:comp-regel})
%
\item How does pruning affect multi-tree's time performance? (\autoref{sec:pruning-split})
%
\item How does %splitting examples and applying 
static multi-tree improve on dynamic multi-tree? (\autoref{sec:pruning-split})
%
\item How does multi-tree compare against other enumeration-based encodings? (\autoref{sec:multi-tree-vs-encodings})
%
\item How many examples are required to return an accurate solution? (\autoref{sec:fewer-exs})
%
\item How does sketching and a multi-distinguish interaction affect \Forest's time performance? (\autoref{sec:res-sketching-interaction})
\end{enumerate}

\input{tables/instances_solved}


\autoref{table:number-solved} shows the number of instances synthesised in under 10, 60 and 3600 seconds using \Forest{}, as well as using the different variations of the synthesizer which will be described in the following sections.
The first row shows the synthesis time using \Forest's default configuration.
\Forest{}, by default, uses static multi-tree when it is possible to split the examples with dividing substrings, and dynamic multi-tree when it is not.
In practice, static multi-tree is used in 57 of the 69 benchmarks (83\%). The default setting has all pruning techniques enabled, does not use sketch-based enumeration, and disambiguates the examples using conversational clarification.
The second row in the table refers to \Forest's synthesis times with no interaction, in which case the returned solution is simply the first one found.
In the third row, we show the synthesis time using the multi-distinguish interaction model.
The following two rows show \Forest's time performance with variations to the enumeration procedure: with pruning disabled and applying dynamic multi-tree enumeration in all benchmarks.
Rows 6~and~7 show synthesis times using different encodings for regex enumeration, \(k\)-tree and line-based.
Finally, we also compare \Forest's synthesis times with \Regel~\cite{Regel20}.
\Regel takes as input a natural description of user intent, used to generate a sketch, as well as examples, which are used to complete the sketch. We show the sketch-completion time of the complete \Regel synthesizer, and synthesis time of the PBE engine of \Regel by itself, which we denote by \Regel PBE. When using the complete \Regel, the synthesis time includes not only sketch-completion times (indicated in the table), but also sketch-generation, which takes on average 60 seconds per benchmark. %\autoref{sec:comp-regel} provides a more detailed comparison between \Forest and \Regel.

\input{figures/cactus}

The cactus plots in \autoref{fig:cactus} show the cumulative synthesis time on the y-axis plotted against the number of benchmarks synthesised by each variation of \Forest{} (on the x-axis).
%
In both plots in \autoref{fig:cactus}, each line corresponds to a row in \autoref{table:number-solved}.
%
The synthesis methods that correspond to lines more to the right of the plot are able to find a solution to more benchmarks in less time. \autoref{fig:cactus_3600} shows the benchmarks synthesised before the 3600 seconds timeout and \autoref{fig:cactus_60} the number solved in the first 60 seconds.

\Forest correctly finds a solution 33 benchmarks (48\%) in under 10 seconds.
In one hour, \Forest{} synthesises 52 benchmarks (75\%), with 98\% accuracy: only one solution did not correspond to the desired regex validation. This happens because \Forest disambiguates only among programs at the same depth. In this instance, the the first solution is at a lower depth as the correct one, thus the correct solution is never found. 
After 1~hour of running time, \Forest{} is interrupted, but it prints its current best validation before terminating.
After the timeout, \Forest{} returned 2 more regexes, both the correct solution for the benchmark. \Forest{} never uses more than 500MB of RAM in each benchmark.
%
In all benchmarks to which \Forest returns a solution, the first matching regular expression is found in under 10 minutes. In 44 benchmarks, the first regex is found in under 10 seconds.  The remaining time is spent looking for better answers and disambiguating the input examples. 
\Forest interacts with the user to disambiguate the examples in 29 benchmarks. Overall, it asks 1.7 questions and spends 37.2 seconds computing distinguishing inputs, on average. 
%
Without interaction, \Forest finds a solution a lot faster. 44 regexes are synthesised in the first ten seconds, 55 after one hour. However, the accuracy is much lower: instead of 51 correct validations, we have only 34 correct validations. This is a severe decrease in accuracy, from 98\% to 62\%.

\Forest is able to synthesise the capturing groups that match the captures provided with each valid example in 9 out of 10 instances. In the 9 instances in which capturing groups are computed successfully, the capturing groups are computed in less than half a second.
In the remaining instance, the first regex \Forest finds that satisfies all examples is not compatible with the required captures.
In this particular situation, the regex should identify HTML-format colours, which consist in a `\#' followed by 6 hexadecimal digits, 3 groups of 2 digits, each referring to a colour component, e.g. `\#0A1B3C'.
Furthermore, the capturing groups should extract the values corresponding to each colour component. For the same example, the desired captures are (`0A',`1B',`3C'). To validate the format, \Forest synthesises the regex \verb`#[0-9A-F]{6}` in under 1 second.
However, there are no capturing groups over this regex that produce the desired captures. To produce the captures, we must first synthesise the regex
\verb`#([0-9A-F]{2})([0-9A-F]{2})([0-9A-F]{2})`, and \Forest does not enumerate that regex within the time limit (1 hour). Therefore, this instance times out.

No instances time out during the synthesis of capture conditions.
In 6 of the benchmarks, we need only 2 capturing groups and at most 4 conditions.
In these 6 instances, the conditions' synthesis takes under 2 seconds.
The remaining 2 benchmarks need 4 capturing groups and take longer.
It takes 99 seconds to synthesise 4 capturing groups with 4 conditions.
The instance that takes the longest to compute capture conditions is quite complex. The synthesised regular expression has 23 nodes, and it requires 6 capture conditions over 4 capturing groups. The result validates a datetime format: \verb`[0-9]{4}/([0-9]{2})/([0-9]{2}) ([0-9]{2}):([0-9]{2}) AM|PM`, \(\$0\le1 \land \$0\le12 \land \$1\le1 \land \$1~\le~31 \land \$2\le12 \land \$3\le59\). This benchmark takes 1229 seconds (about 20 minutes) to synthesise, 97\% of which is spent synthesising the capture conditions.
%
During capture conditions synthesis, \Forest interacts 6.75 times and takes 0.1 seconds to compute distinguishing inputs, on~average.

%\afterpage{\clearpage} % makes all floats before this point be shown at the page after current.

\section{Comparison with \Regel}\label{sec:comp-regel}

\input{figures/time_comp_regel}

As mentioned in \autoref{sec:rel-regex-synthesizers}, \Regel's synthesis procedure is split into two steps: sketch generation (using a natural language description of desired behaviour) and sketch completion (using input-output examples).
%
To compare \Regel and \Forest{}, we extended our 69 form validation benchmarks with a natural language description.
To assess the importance of the natural language description, we also ran \Regel using only its PBE engine. %without sketches.
%
In \autoref{fig:forest-vs-regel} we can compare \Forest's total synthesis time with \Regel's sketch completion time. Each mark in the plot represents an instance. The value on the y-axis shows \Regel's sketch completion time and the corresponding value on the x-axis is \Forest's synthesis time for the same instance. The marks above the \(y = x\) line (also represented in the plot) represent problems that took longer to synthesise with \Regel than with \Forest.
\autoref{fig:forest-vs-regel-pbe} shows \Forest's total synthesis time against \Regel-PBE's synthesis time.
Sketch generation took on average 60 seconds per instance, and successfully generated a sketch for 68 instances. The remaining instance was run without a sketch.
We considered only the highest ranked sketch for each instance.
In \autoref{table:number-solved} we show how many instances can be solved with different time limits for sketch completion; note that these values do not include the sketch generation time.
%
\Regel{} returned a regular expression for 53 instances and timed out after one hour in the remaining 16.
Since \Regel does not implement a disambiguation procedure, the returned regular expression does not always exhibit the desired behaviour, even though it correctly classifies all examples.
Of the 53 synthesised expressions, 35 exhibit the desired~intent (only one more than \Forest's first solution).
This is a 66\% accuracy, which is much lower than \Forest's at 96\%. 
We conclude that, in terms of synthesis time, \Regel is comparable to \Forest, which solves only one fewer instance. However, \Regel's accuracy for the desired validation is much lower than that of \Forest, as they don't implement an interaction model. In terms of accuracy, \Regel is comparable to \Forest's \nth{1} solution, which has lower synthesis times in general.

55 out of the 68 generated sketches are of the form \(\square\{S_1, ..., S_n\}\),
where each \(S_i\) is a concrete sub-regex, i.e., has no holes.
This construct indicates the desired regex must contain \textit{at least} one of \(S_1, ..., S_n\),
and contains no information about the top-level operators that are used to connect them.
%
24 of the 53 synthesised regexes are based on sketches of that form, and they result from the direct concatenation of \textit{all} components in the sketch. No new components are generated during sketch completion.
%
Thus, most of \Regel{}'s sketches could be integrated into \Forest{}, whose multi-tree structure holds precisely those top-level operators that were missing from \Regel's~sketches. As can be seen in \autoref{fig:forest-vs-regel-pbe} and \autoref{table:number-solved}, \Regel's performance is severely impaired when using only its PBE~engine. We predict integrating \Regel{}'s sketches with \Forest would have a positive outcome, since \Forest shows better performance working only from examples.

\section{Pruning the Search Space and Splitting Examples}\label{sec:pruning-split}

\input{figures/time_comp_multitree}

To evaluate the impact of pruning the search space as described in \autoref{sec:pruning}, we ran \Forest{} with all pruning techniques disabled.
In the scatter plot in \autoref{fig:mt_vs_nopruning}, we can compare the synthesis time on each benchmark with and without pruning.
On average, with pruning, \Forest{} can synthesise regexes in half the time and enumerates less than half of the regexes before returning. There is no significant change in the number of interactions before returning the desired~solution.

\Forest{} is able to split the examples and use static multi-tree as described in \autoref{sec:static-multi-tree} in 57 benchmarks (83\%).
The remaining 12 are solved using dynamic multi-tree.
To assess the impact of using static multi-tree we ran \Forest{} with a version of the multi-tree enumerator that does not attempt to split the examples, and jumps directly to dynamic multi-tree enumeration.
In the scatter plot in \autoref{fig:mt_vs_dynamic}, we compare the synthesis times of each benchmark.
Using static multi-tree when possible, \Forest{} requires, on average, about two thirds of the time (68\%) to return the desired regex for the benchmarks solved by both methods.
Furthermore, static multi-tree allows \Forest{} to synthesise more complex expressions: the maximum number of nodes in a solution returned by dynamic multi-tree is 12 (average 6.7), while complete multi-tree synthesises expressions of up to 24 nodes (average 10.2).



\section{Multi-tree versus \texorpdfstring{\textit{k}-tree}{k-tree} and Line-based Encodings}\label{sec:multi-tree-vs-encodings}

\input{figures/time_comp_encodings}

To evaluate the performance of the multi-tree encoding, we ran \Forest{} with two other enumeration encodings: \(k\)-tree and line-based.
The latter is a state of the art encoding for the synthesis of SQL queries \cite{Orvalho19,DBLP:journals/pvldb/OrvalhoTVMM20}.
\(k\)-tree is implemented as the default enumerator in \textsc{Trinity}~\cite{trinity19}, while the line-based enumerator is used as available in \textsc{Squares}~\cite{squares-webpage}.
The \(k\)-tree encoding (introduced in \autoref{sec:k-tree}) has a very similar structure to that of multi-tree, so our pruning techniques were easily applied to this encoding.
%
On the other hand, line-based encoding is intrinsically different, making the pruning techniques harder to implement. Because of this, we compare line-based encoding to multi-tree without pruning.
%
In every other aspect, the three encodings were run in the same conditions, using \Forest{}'s regex \ac{DSL}.
The synthesis time comparison of multi-tree with \(k\)-tree and line-based encodings are shown in the scatter plots in \autoref{fig:mt_vs_ktree} and \autoref{fig:mt_vs_lines}, respectively.
\(k\)-tree is able to synthesise programs with up to 10 nodes, while the line-based encoding synthesises programs of up to 9 nodes. Neither encoding~outperforms~multi-tree.

As can be seen in \autoref{table:number-solved} and \autoref{fig:cactus}, line-based encoding does not outperform the tree-based encodings in our experiments. This contradicts the results obtained by \citet{Orvalho19} for the synthesis of SQL queries. We conjecture the reason behind this disparity arises from different nature of the DSLs. Most SQL queries, when represented as a tree, leave many branches of the tree unused, which results in the need of a larger depth for the tree. Regular expressions, on the other hand, take more advantage of the tree structure.

\FloatBarrier
\section{Fewer Examples} \label{sec:fewer-exs}

To assess the impact of providing fewer examples on the accuracy of the solution, we ran \Forest{} with modified versions of each benchmark.
First, each benchmark was run with at most 10 valid and 10 invalid examples, chosen randomly among all examples. Conditional invalid examples are already very few per instance, so these were not altered. The accuracy of the returned regexes is slightly~lower.

With only 10 valid and 10 invalid examples, \Forest{} returns the correct regex in 96\% of the benchmarks, which represents a very slight decrease compared with results with all examples, where the accuracy was 98\%.
We also saw an increase in the number of interactions before returning, since fewer examples are likely to be more ambiguous. With only 10 examples, \Forest{} interacts on average 2.1 times per instance during regex synthesis, which represents an increase of about a fourth compared to \Forest run with all examples, which interacted on average 1.7 times per instance.

After, we reduced the number of examples even further: only 5 valid and 5 invalid. The accuracy of \Forest{} in this setting was reduced to 71\%. On average, it interacted 4.0 times per benchmark, which is over two times more than~before.


\section{Sketching and Multi-distinguish Interaction}
\label{sec:res-sketching-interaction}

We implemented sketch-based enumeration in \Forest with both sketch-completion methods described in \autoref{sec:regex-sketches}: graph-based and \ac{SMT}-based.
%
Graph-based sketch completion solved only 4 instances, all in under 1 second. All 4 successfully synthesised instances are among the simplest in the benchmark set, the synthesised regex has only 4 nodes, and the correct sketch was enumerated within the first 6 sketches. This means \Forest had to exhaust very few sketches (at most 5) before finding the correct one. In sum, the only instances that are solved with graph-based sketching are those where the regex is simplest and whose correct sketches happens to be enumerated first. In the remaining instances, \Forest enumerates and tests at most 500 sketches before timeout, while sketch-less \Forest enumerates at most 30 thousand regexes before the timeout. Graph-based sketch completion is inefficient because the possible values for each leaf are not pruned, and all combinations are tested.

\ac{SMT}-based sketching performed only slightly better: 8 instances were solved in total. Synthesis times vary in accordance with each solution's complexity. Four 4-node regexes synthesised in under 2 seconds, one 5- and one 6-node regex synthesised in under one minute, and the remaining two 8-node solutions in under one hour. Of the unsolved instances, only one timed out. The remaining 47 were forcefully terminated due to insufficient memory within the imposed 4GB limit. We observed out-of-memory errors even with the limit set as high as 32GB. This happens because the \ac{SMT} formula grows exponentially with the number of holes in the sketch, so larger sketches quickly become intractable.

We also compared the behaviour of the two interaction models described for regex synthesis in \autoref{sec:regex-interaction}.
The multi-distinguish model was run every 4 regexes found, because
our experiments showed that Z3's regex theory could not efficiently distinguish more than 4 regexes at a time.
%
Regardless of the interaction model, \Forest interacts with the user in 29 benchmarks. With multi-distinguish, of these 29, only 23 successfully terminate synthesis and compute a regex.
%
One of the failed instances times out before enumerating the correct regex. The remaining 5 timeout after finding 4 regexes in the first minute of synthesis and spending the remaining 59 minutes unsuccessfully trying to find a distinguishing input for them.
While 4 instances we distinguishing relatively complex regexes (10-16 nodes), the remaining one times out trying to distinguish simple 4-node regular expressions.

Regarding the number of interactions, 13 instances required 3 or more interactions using the multi-distinguish model, which is 3 fewer than with conversational clarification.
With multi-distinguish, \Forest required at most 9 regex interactions before converging to a solution. With conversational clarification, \Forest interacts up to 21 times to synthesise the correct regular expression.
Overall, considering only benchmarks synthesised in both runs, with conversational clarification \Forest spends about half the time (3.2 seconds on average) computing distinguishing input than with multi-distinguish (6.2 seconds on average).

As expected, when maximising the number of programs eliminated by each interactions, \Forest had on average fewer interactions with the user.
Although it successfully minimises the number of required interactions to completely disambiguate the specification, we conclude that multi-distinguish is not a viable alternative to conversational clarification for two motives:
\begin{enumerate*}[label=(\roman*)]
    \item it takes a lot longer to compute a distinguishing input, even with as few as 4 programs to distinguish, and
    \item Z3's regex theory is unpredictable: it efficiently distinguishes some complex regexes but also fails with simple ones.
\end{enumerate*}
Thus, \Forest uses conversational clarification to interact with the user by default.
