\chapter{Experimental Results}\label{chap:results}

\paragraph{Implementation.}
\Forest{} is implemented in Python 3.8 on top of \textsc{Trinity}, a general-purpose synthesis framework \cite{trinity19}.
%\textsc{Trinity} was extended to support our Regular Expression \ac{DSL}, multi-tree constructs, and interaction based on distinguishing inputs.
All \ac{SMT} formulas are solved using the Z3 SMT solver, version  4.8.9 \cite{z3}. 
To find distinguishing inputs in regular expression synthesis, \Forest uses Z3's theory of regular expressions (part of the theory of strings \cite{z3str317}).
To check the enumerated regexes against the examples, we use Python's regex library \cite{PythonRe}.
%
The results presented herein were obtained using an Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz, with 64GB of RAM, running Debian GNU/Linux~10. All processes were run with a time limit of one hour.

\paragraph{Benchmarks.} To evaluate \Forest{}, we used 69 benchmarks based on real-world form-validation regular expressions. These were collected from regular expression validators in validation frameworks and from \texttt{regexlib}~\cite{regexlib}, a website where users can upload their own regexes. Among these 69 benchmarks there are different formats: national IDs, identifiers of products, date and time, vehicle registration numbers, postal codes, email and phone numbers.
For each benchmark, we randomly generated sets of strings to be used as input examples for \Forest{}. 
%All benchmarks' goal is to produce a regex validation.
All 69 benchmarks require a regular expression to validate the examples. 10 require capturing groups to extract the captures provided alongside the valid examples, and 8 require additional capture conditions to validate conditional invalid examples.
On average, each instance is composed of 13.4 valid examples (ranging from 4 to 33) and 9.0 invalid (ranging from 2 to 38). The 7 instances that target capture conditions have on average 6 conditional invalid examples (ranging from 4 to 8). 
The median number of total examples per instance is 22, while the median number of valid examples is 11, 7 for invalid, and 6 for conditional invalid.

\bigskip\noindent
The goal of this experimental evaluation is to answer the following questions:

\begin{enumerate}[label=\textbf{Q\arabic*}.]
%
\item How does \Forest{} compare against \Regel? (\autoref{sec:comp-regel})
%
\item How does pruning affect multi-tree's time performance? (\autoref{sec:pruning-split})
%
\item How does %splitting examples and applying 
static multi-tree improve on dynamic multi-tree? (\autoref{sec:pruning-split})
%
\item How does multi-tree compare against other enumeration-based encodings? (\autoref{sec:multi-tree-vs-encodings})
%
\item How many examples are required to return an accurate solution? (\autoref{sec:fewer-exs})
%
\item How does sketching and a different interaction model affect \Forest's time performance? (\autoref{sec:res-sketching-interaction})
\end{enumerate}

\input{tables/instances_solved}
\input{figures/cactus}

\Forest{}, by default, uses static multi-tree (when possible) with pruning. It does not use any sketching technique, and disambiguates the examples using conversational clarification. It correctly solves 33 benchmarks (48\%) in under 10 seconds.
In one hour, \Forest{} solves 52 benchmarks (75\%), with 98\% accuracy: only one solution did not correspond to the desired regex validation. This happens because \Forest disambiguates only among programs at the same depth. In this instance, the the first solution is at a lower depth as the correct one, thus the correct solution is never found. 
After 1~hour of running time, \Forest{} is interrupted, but it prints its current best validation before terminating.
After the timeout, \Forest{} returned 2 more regexes, both the correct solution for the benchmark. \Forest{} never uses more than 500MB of RAM in each benchmark.
%
In all benchmarks to which \Forest returns a solution, the first matching regular expression is found in under 10 minutes. In 44 benchmarks, the first regex is found in under 10 seconds.  The remaining time is spent looking for better answers and disambiguating the input examples. 
\Forest interacts with the user to disambiguate the examples in 29 benchmarks. Overall, it asks 1.7 questions and spends 37.2 seconds computing distinguishing inputs, on average. 

\Forest is able to synthesise the capturing groups that match the captures provided with each valid example in 9 out of 10 instances. In the 9 instances in which capturing groups are computed successfully, the capturing groups are computed in less than half a second.
In the remaining instance, the first regex \Forest finds that satisfies all examples is not compatible with the required captures.
In this particular situation, the regex should identify HTML-format colours, which consist in a `\#' followed by 6 hexadecimal digits, 3 groups of 2 digits, each referring to a colour component, e.g. `\#0A1B3C'.
Furthermore, the capturing groups should extract the values corresponding to each colour component. For the same example, the desired captures are (`0A',`1B',`3C'). To validate the format, \Forest synthesises the regex \verb`#[0-9A-F]{6}` in under 1 second.
However, there are no capturing groups over this regex that produce the desired captures. To produce the captures, we must first synthesise the regex
\verb`#([0-9A-F]{2})([0-9A-F]{2})([0-9A-F]{2})`, and \Forest does not enumerate that regex within the time limit (1 hour). Therefore, this instance times out.

During the synthesis of capture conditions no instances time out. In 2 of the benchmarks, we need only 2 capturing groups and at most 2 conditions. In 4 of the benchmarks, we need only 2 capturing groups and at most 4 conditions. In these 6 instances, the conditions' synthesis takes under 2 seconds.
The remaining 2 benchmarks need 4 capturing groups and take longer: 99 seconds to synthesise 4 conditions and 1068 seconds for 6 conditions. During capture conditions synthesis, \Forest interacts 6.75 times and takes 0.1 seconds to compute distinguishing inputs, on~average.

\autoref{table:number-solved} shows the number of instances solved in under 10, 60 and 3600 seconds using \Forest{}, as well as using the different variations of the synthesizer which will be described in the following sections. The cactus plot in \autoref{fig:cactus} shows the cumulative synthesis time on the y-axis plotted against the number of benchmarks solved by each variation of \Forest{} (on the x-axis). The synthesis methods that correspond to lines more to the right of the plot are able to solve more benchmarks in less time. \autoref{fig:cactus_3600} shows the benchmarks solved before the 3600 seconds timeout and \autoref{fig:cactus_60} the number solved in the first 60 seconds.
%
We also compare solving times with \Regel~\cite{Regel20}.
\Regel takes as input examples and a natural description of user intent. We consider not only the complete \Regel synthesizer, but also the PBE engine of \Regel by itself, which we denote by \Regel PBE.

\afterpage{\clearpage} % makes all floats before this point be shown at the page after current.

\section{Comparison with \Regel}\label{sec:comp-regel}

\input{figures/time_comp_regel}

As mentioned in \autoref{sec:rel-regex-synthesizers}, \Regel's synthesis procedure is split into two steps: sketch generation (using a natural language description of desired behaviour) and sketch completion (using input-output examples).
%
To compare \Regel and \Forest{}, we extended our 69 form validation benchmarks with a natural language description.
To assess the importance of the natural language description, we also ran \Regel using only its PBE engine. %without sketches.
%
In \autoref{fig:forest-vs-regel} we can compare \Forest's total synthesis time with \Regel's sketch completion time. Each mark in the plot represents an instance. The value on the y-axis shows \Regel's sketch completion time and the corresponding value on the x-axis is \Forest's synthesis time for the same instance. The marks above the \(y = x\) line (also represented in the plot) represent problems that took longer to synthesise with \Regel than with \Forest.
\autoref{fig:forest-vs-regel-pbe} shows \Forest's total synthesis time against \Regel-PBE's synthesis time.
Sketch generation took on average 60 seconds per instance, and successfully generated a sketch for 68 instances. The remaining instance was run without a sketch.
We considered only the highest ranked sketch for each instance.
In \autoref{table:number-solved} we show how many instances can be solved with different time limits for sketch completion; note that these values do not include the sketch generation time.
%
\Regel{} returned a regular expression for 53 instances and timed out after one hour in the remaining 16.
Since \Regel does not implement a disambiguation procedure, the returned regular expression does not always exhibit the desired behaviour, even though it correctly classifies all examples.
Of the 53 synthesised expressions, 35 exhibit the desired~intent (only one more than \Forest's first solution).
This is a 66\% accuracy, which is much lower than \Forest's at 96\%. 
We conclude that, in terms of solving time, \Regel is comparable to \Forest, which solves only one fewer instance. However, \Regel's accuracy for the desired validation is much lower than that of \Forest, as they don't implement an interaction model. In terms of accuracy, \Regel is comparable to \Forest's \nth{1} solution, which has lower synthesis times in general.

55 out of the 68 generated sketches are of the form \(\square\{S_1, ..., S_n\}\),
where each \(S_i\) is a concrete sub-regex, i.e., has no holes.
This construct indicates the desired regex must contain \textit{at least} one of \(S_1, ..., S_n\),
and contains no information about the top-level operators that are used to connect them.
%
24 of the 53 synthesised regexes are based on sketches of that form, and they result from the direct concatenation of \textit{all} components in the sketch. No new components are generated during sketch completion.
%
Thus, most of \Regel{}'s sketches could be integrated into \Forest{}, whose multi-tree structure holds precisely those top-level operators that were missing from \Regel's~sketches. As can be seen in \autoref{fig:forest-vs-regel-pbe} and \autoref{table:number-solved}, \Regel's performance is severely impaired when using only its PBE~engine. We predict integrating \Regel{}'s sketches with \Forest would have a positive outcome, since \Forest shows better performance working only from examples.

\section{Pruning the Search Space and Splitting Examples}\label{sec:pruning-split}

\input{figures/time_comp_multitree}

To evaluate the impact of pruning the search space as described in \autoref{sec:pruning}, we ran \Forest{} with all pruning techniques disabled.
In the scatter plot in \autoref{fig:mt_vs_nopruning}, we can compare the solving time on each benchmark with and without pruning.
On average, with pruning, \Forest{} can synthesise regexes in half the time and enumerates less than half of the regexes before returning. There is no significant change in the number of interactions before returning the desired~solution.

\Forest{} is able to split the examples and use static multi-tree as described in \autoref{sec:static-multi-tree} in 57 benchmarks (83\%).
The remaining 12 are solved using dynamic multi-tree.
To assess the impact of using static multi-tree we ran \Forest{} with a version of the multi-tree enumerator that does not attempt to split the examples, and jumps directly to dynamic multi-tree solving.
In the scatter plot in \autoref{fig:mt_vs_dynamic}, we compare the solving times of each benchmark.
Using static multi-tree when possible, \Forest{} requires, on average, about two thirds of the time (68\%) to return the desired regex for the benchmarks solved by both methods.
Furthermore, static multi-tree allows \Forest{} to synthesise more complex expressions: the maximum number of nodes in a solution returned by dynamic multi-tree is 12 (average 6.7), while complete multi-tree synthesises expressions of up to 24 nodes (average 10.2).



\section{Multi-tree versus \texorpdfstring{\textit{k}-tree}{k-tree} and Line-based Encodings}\label{sec:multi-tree-vs-encodings}

\input{figures/time_comp_encodings}

To evaluate the performance of the multi-tree encoding, we ran \Forest{} with two other enumeration encodings: \(k\)-tree and line-based.
The latter is a state of the art encoding for the synthesis of SQL queries \cite{Orvalho19,DBLP:journals/pvldb/OrvalhoTVMM20}.
\(k\)-tree is implemented as the default enumerator in \textsc{Trinity}~\cite{trinity19}, while the line-based enumerator is used as available in \textsc{Squares}~\cite{squares-webpage}.
The \(k\)-tree encoding has a very similar structure to that of multi-tree, so our pruning techniques were easily applied to this encoding.
%In the scatter plot in \autoref{fig:mt_vs_ktree}, we can compare the solving time on each benchmark with \Forest's multi-tree encoding and \(k\)-tree.
On the other hand, line-based encoding is intrinsically different, making the pruning techniques harder to implement. Because of this, we compare line-based encoding to multi-tree without pruning.
%\autoref{fig:mt_vs_lines}, we can compare the solving time on each benchmark with \Forest's multi-tree with pruning disabled and line-based encoding.
In every other aspect, the three encodings were run in the same conditions, using \Forest{}'s regex \ac{DSL}.
The synthesis time comparison of multi-tree with \(k\)-tree and line-based encodings are shown in the scatter plots in \autoref{fig:mt_vs_ktree} and \autoref{fig:mt_vs_lines}, respectively.
\(k\)-tree is able to synthesise programs with up to 10 nodes, while the line-based encoding synthesises programs of up to 9 nodes. Neither encoding~outperforms~multi-tree.

As seen in \autoref{table:number-solved}, line-based encoding does not outperform the tree-based encodings for the domain of regular expressions while it was much better for the domain of SQL queries~\cite{Orvalho19}. We conjecture the reason behind this disparity arises from the different nature of \ac{DSL}s. Most SQL queries, when represented as a tree, leave many branches of the tree unused and have more children per node, which results in a much larger tree and SMT encoding.

As can be seen in \autoref{table:number-solved} and \autoref{fig:cactus}, line-based encoding does not outperform the tree-based encodings in our experiments. This contradicts the results obtained by \citet{Orvalho19} for the synthesis of SQL queries. We conjecture the reason behind this disparity arises from different nature of the DSLs. Most SQL queries, when represented as a tree, leave many branches of the tree unused, which results in the need of a larger depth for the tree. Regular expressions, on the other hand, take more advantage of the tree structure.

\FloatBarrier
\section{Fewer Examples} \label{sec:fewer-exs}

To assess the impact of providing fewer examples on the accuracy of the solution, we ran \Forest{} with modified versions of each benchmark.
First, each benchmark was run with at most 10 valid and 10 invalid examples, chosen randomly among all examples. Conditional invalid examples are already very few per instance, so these were not altered. The accuracy of the returned regexes is slightly~lower.

With only 10 valid and 10 invalid examples, \Forest{} returns the correct regex in 93.5\% of the benchmarks, which represents a decrease of only 2.5\% relative to the results with all examples.
We also saw an increase in the number of interactions before returning, since fewer examples are likely to be more ambiguous. With only 10 examples, \Forest{} interacts on average 2.2 times per benchmark, which represents an increase of about a fifth.
The increase in the number of interactions reflects on a small increase in the synthesis time (less than 1\%).

After, we reduced the number of examples even further: only 5 valid and 5 invalid. The accuracy of \Forest{} in this setting was reduced to 71\%. On average, it interacted 4.3 times per benchmark, which is over two times more than~before.


\section{Sketching and Multi-distinguish Interaction}
\label{sec:res-sketching-interaction}

We implemented sketch-based enumeration in \Forest with both sketch-completion methods described in \autoref{sec:regex-sketches}: graph-based and \ac{SMT}-based.
%
Graph-based sketch completion solved only 4 instances, all in under 1 second. All 4 instances are among the simplest in the benchmark set: the synthesised regex has only 4 nodes. In these 4 instances, the correct sketch was enumerated within the first 6 sketches. This means \Forest had to exhaust at most 5 sketches before finding the correct one. In sum, the only instances that are solved with graph-based sketching are those where the regex is simplest and whose correct sketches happens to be enumerated first. In the remaining instances, \Forest enumerates and tests at most 500 sketches before timeout, while sketch-less \Forest enumerates at most 30 thousand regexes before the timeout. Graph-based sketch completion is inefficient because the possible values for each leaf are not pruned, and all combinations are tested.

\ac{SMT}-based sketching performed only slightly better: 8 instances were solved in total. Synthesis times vary in accordance with each solution's complexity. Four 4-node regexes synthesised in under 2 seconds, one 5- and one 6-node regex synthesised in under one minute, and the remaining two 8-node solutions in under one hour. Of the unsolved instances, only one timed out. The remaining 47 were forcefully terminated due to insufficient memory (within the imposed 4GB limit). This happens because the \ac{SMT} formula grows exponentially with the number of holes in the sketch, so larger sketches quickly become intractable.

We also compared the behaviour of the two interaction models described for regex synthesis in \autoref{sec:regex-interaction}.
The multi-distinguish model was run every 4 regexes found, because
%\begin{enumerate*}[label=(\roman*)]
%    \item
our experiments showed that Z3's regex theory could not efficiently distinguish more than 4 regexes at a time. %, and 
%    \item 4 is the minimum number of regexes to distinguish such that it still makes sense to apply multi-distinguish: ideally we would remove 2 programs from the search space with each interaction
%\end{enumerate*} 

Regardless of the interaction model, \Forest interacts with the user in 29 benchmarks. With multi-distinguish, of these 29, only 23 successfully terminate synthesis and compute a regex. One times out before enumerating the correct regex. The other 5 timeout after finding 4 regexes in the first minute of synthesis and spending the remaining 59 minutes unsuccessfully trying to find a distinguishing input for them.
Of these 5, 4 are distinguishing relatively complex regex (10-16 nodes). The remaining one, however, times out trying to distinguish between four 4-node regular expressions.
13 instances required 3 or more interactions using the multi-distinguish model, which is fewer than with conversational clarification, in which 16 instances require 3 or more interactions and 3 instances require more than 10 interactions. With multi-distinguish, \Forest required at most 9 regex interactions before converging to a solution. With conversational clarification, \Forest interacts up to 21 to synthesise the correct regular expression.
Overall, considering only benchmarks solved in both runs, with conversational clarification \Forest spends about half the time (3.2 seconds on average) computing distinguishing input than with multi-distinguish (6.2 seconds on average).

As expected, when maximising the number of programs eliminated by each interactions, \Forest had on average fewer interactions with the user.
Although it successfully minimises the number of required interactions to completely disambiguate the specification, we conclude that multi-distinguish is not a viable alternative to conversational clarification for two motives:
\begin{enumerate*}[label=(\roman*)]
    \item it takes a lot longer to compute a distinguishing input, even with as few as 4 programs to distinguish, and
    \item Z3's regex theory is unpredictable: it efficiently distinguishes some complex regexes but also fails with simple ones.
\end{enumerate*}
Thus, \Forest uses conversational clarification to interact with the user by default.
