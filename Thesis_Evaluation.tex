\chapter{Evaluation}

\section{Experimental Results}

\paragraph{Implementation.}
\Forest{} is implemented in Python3.7+ on top of \textsc{Trinity}, a general-purpose synthesis framework \cite{trinity19}. \textsc{Trinity} was extended to support our Regular Expression \ac{DSL}, multi-tree constructs, and interaction based on distinguishing inputs. All \ac{SMT} formulas are solved using the Z3 SMT solver, version  4.8.7.0 \cite{z3}. The \textit{enumerator} uses the theory of linear integer arithmetic and the \textit{distinguisher} uses the theory of regular expressions (part of the theory of strings \cite{z3str317}). The \textit{verifier} checks the enumerated regex against the examples using Python's regex library.
%
The results presented herein were obtained using an Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz, with 64GB of RAM, running Debian GNU/Linux~10. All processes were run with a time limit of one hour.

\paragraph{Benchmarks.} To test \Forest{}, we used 64 benchmarks based on real-world form-validation regular expressions. These were collected from regular expression validators in validation frameworks and from \texttt{regexlib}~\cite{regexlib}, a website where users can upload their own regexes. Among these 64 benchmarks there are different formats: national IDs, identifiers of products, date and time, vehicle registration numbers, postal codes, email and phone numbers.

For each benchmark, we randomly generated a set of strings that match the respective regular expression. These strings constitute the valid examples. Next, we manually altered some of the valid examples so that they were no longer matched by the regular expression, thus collecting invalid examples. This set of valid and invalid examples was then used as input for \Forest{}.
On average, each instance is composed of 23 examples, 13.5 valid and 9.5 invalid. The median number of total examples per instance is 21, while the median number of valid examples is 12, and 7 for invalid.

\medskip\noindent
The goal of this experimental evaluation is to answer the following questions:
\begin{enumerate}[topsep=3pt, label=\textbf{Q\arabic*}.]
\item How does pruning affect multi-tree's time performance? (\autoref{sec:pruning-split})
%
\item How does splitting examples and applying static multi-tree improve on dynamic multi-tree? (\autoref{sec:pruning-split})
%
\item How does multi-tree compare to \(k\)-tree and line-based encodings? (\autoref{sec:multi-tree-vs-encodings})
%
\item How does reducing the number of provided examples affect the accuracy of the returned solution? (\autoref{sec:fewer-exs})
%
\item How does \Forest{} compare against \Regel? (\autoref{sec:comp-regel})
\end{enumerate}


\input{tables/instances_solved}
\input{figures/time_comparison_all}
%\input{tables/double_table}

\Forest{}, by default, uses pruning and static multi-tree when possible. It solves 55\% of the benchmarks in under 10 seconds. In one hour, \Forest{} solves 75\% of all benchmarks, with 98\% accuracy (only one solution was not the desired regular expression). After 1 hour of running time, \Forest{} is interrupted and prints its current best expression before terminating. After the timeout, \Forest{} returned 5 more regexes, 2 of which were the correct solution for the benchmark.
\Forest{} interacts on average 1.9 times per benchmark and always uses less than 500MB of RAM in each benchmark.

\autoref{table:number-solved} shows the number of instances solved in under 10 seconds, 1 minute and 1 hour using \Forest{}, as well as using the different variations of the synthesizer which will be described in the following sections. The cactus plot in \autoref{fig:comparison_all_methods} shows the cumulative synthesis time on the y-axis plotted against the number of benchmarks solved by each variation of \Forest{} (on the x-axis). The synthesis methods that correspond to lines more to the right of the plot are able to solve more benchmarks in less time. 
%
We also compare solving times with \Regel~\cite{Regel20}. Even though \Regel can take as input examples and a natural description of user intent, we only consider the PBE engine of \Regel, which we denote by \Regel PBE. This is done to have a fair comparison with our synthesizer and because we do not have a natural language description for our benchmarks.%, we only used the PBE engine of \Regel{}, which we denote by \Regel{} PBE.

% We also compared solving times with \Regel{} PBE, proposed by Chen et. al. \cite{Regel20}.
% %
% \Regel{} is a state-of-the-art synthesizer of regular expressions. Its synthesis procedure uses a multi-modal approach that combines input-output examples with a natural language description of user intent.
% %
% However, to perform a fair comparison with our synthesizer and because we do not have a natural language description (NLP) for our benchmarks, we only used the PBE engine of \Regel{}, which we denote by \Regel{} PBE.


\subsection{Impact of pruning the search space and splitting examples}\label{sec:pruning-split}
%\input{figures/time_comparison_pruning}
\input{figures/time_comparison_dynamic}

To evaluate the impact of pruning the search space as described in \autoref{sec:pruning}, we ran \Forest{} with all pruning techniques disabled.
In the scatter plot in \autoref{fig:mt_vs_nopruning}, we can compare the solving time on each benchmark with and without pruning. Each mark in the plot represents an instance. The value on the y-axis shows the synthesis time of multi-tree with pruning disabled and the value on the x-axis the synthesis time with pruning enabled. The marks above the \(y = x\) line (also represented in the plot) represent problems that took longer to synthesize without pruning than with pruning.
On average, with pruning, \Forest{} can synthesize regexes in half the time and enumerates less than half of the regexes before returning. There is no significant change in the number of interactions before returning the desired solution.


%To measure the impact of splitting the examples and having a fixed number of sub-trees, each with its own reduced \ac{DSL}, we ran an alternative version of the synthesizer where the splitting step is skipped and the synthesizer always uses the dynamically growing version of the encoding.
\Forest{} is able to split the examples and use static multi-tree as described in \autoref{sec:static-multi-tree} in over 80\% of the benchmarks. The remaining 20\% are solved using dynamic multi-tree. To assess the impact of using static multi-tree we ran \Forest{} with a version of the multi-tree enumerator that does not attempt to split the examples, and jumps directly to dynamic multi-tree solving.
In the scatter plot in \autoref{fig:mt_vs_dynamic}, we compare the solving times of each benchmark. Using static multi-tree when possible, \Forest{} requires, on average, less than one third of the time (27.36\%) to return the desired regex. Furthermore, static multi-tree allows \Forest{} to synthesize more complex expressions: the maximum number of nodes in a solution returned by dynamic multi-tree is 13 (average 6.3), while complete multi-tree synthesizes expressions of up to 24 nodes (average 11.2).

\subsection{Multi-tree versus \(k\)-tree and line-based encodings}\label{sec:multi-tree-vs-encodings}
%\input{figures/time_comparison_encoding}

To evaluate the performance of the multi-tree encoding, we ran \Forest{} with two other enumeration encodings: \(k\)-tree and line-based.
The latter is a state of the art encoding for the synthesis of SQL queries \cite{Orvalho19}. \(k\)-tree is implemented as the default enumerator in \textsc{Trinity}~\cite{trinity19}, while the line-based enumerator is used as available in \textsc{Squares}~\cite{squares-webpage}. The \(k\)-tree encoding has a very similar structure to that of multi-tree, so our pruning techniques were easily applied to this encoding. On the other hand, line-based encoding is intrinsically different, making the pruning techniques harder to implement. Because of this, we compare line-based encoding to multi-tree without pruning.
In every other aspect, the three encodings were run in the same conditions, using \Forest{}'s regex \ac{DSL}.
%The synthesis time comparison of multi-tree with \(k\)-tree and line-based encodings are shown in \autoref{fig:mt_vs_ktree} and \autoref{fig:mt_vs_lines}, respectively.
%Considering only benchmarks that we solved with both methods, we observed that \(k\)-tree takes on average 6 times longer to return a regular expression, while 
\(k\)-tree is able to synthesize programs with up to 15 nodes, while the line-based encoding synthesizes programs of up to 9 nodes. Neither encoding~outperforms~multi-tree. %in terms of time performance.

As seen in \autoref{table:number-solved}, line-based encoding does not outperform the tree-based encodings for the domain of regular expressions while it was much better for the domain of SQL queries~\cite{Orvalho19}. We conjecture the reason behind this disparity arises from the different nature of DSLs. Most SQL queries, when represented as a tree, leave many branches of the tree unused and have more children per node, which results in a much larger tree and SMT encoding.

%As can be seen in \autoref{table:number-solved}, line-based encoding does not outperform the tree-based encodings in our experiments. This contradicts the results obtained by Orvalho et. al. for the synthesis of SQL queries. We conjecture the reason behind this disparity arises from different nature of the DSLs. Most SQL queries, when represented as a tree, leave many branches of the tree unused, which results in the need of a larger depth for the tree. Regular expressions, on the other hand, are able better leverage the tree structure.

\subsection{Impact of fewer examples} \label{sec:fewer-exs}
To assess the impact of providing fewer examples on the accuracy of \Forest{}'s solution, we ran \Forest{} with modified versions of each benchmark.
First, each benchmark was run with only 10 valid and 10 invalid examples, chosen randomly among all examples. The accuracy of the returned regexes is slightly lower.
With only 10 valid and 10 invalid examples, \Forest{} returns the correct regular expression in 94\% of the benchmarks, which represents a decrease of only 4\% relative to the results with all examples.
We also saw an increase in the number of interactions before returning, since fewer examples are likely to be more ambiguous. With only 10 examples, \Forest{} interacts on average 2.6 times per benchmark, which represents an increase of about a third. The increase in the number of interactions reflects on a small increase in the synthesis time (less than 10\%). There was no significant increase in the number of enumerated expressions (1\%).

After, we reduced the number of examples even further: only 5 valid and 5 invalid. The accuracy of \Forest{} in this setting was reduced to 82\%. On average, it interacted 6.1 times per benchmark, which is 3 times more than before. It also enumerated 25\% more expressions before returning.

%only 82\%, 16\% less than with all examples. On average, it interacted 6.1 times per benchmark, which is 3 times more than before. It also enumerated 25\% more expressions before returning.

\subsection{Comparison with \Regel}\label{sec:comp-regel}
%\input{figures/time_comparison_resnax}

\input{tables/instances_solved_resnax}

We run \Regel{} PBE on our 64 regex benchmarks for form validation.
%
\Regel{} returned a regular expression in 27 instances, timed out after 1 hour in 19, and terminated due to insufficient memory (32GB) in the remaining 18. Of the 27 synthesized expressions, only 8 satisfied the desired intent.

We also ran \Forest{} and \Regel{} PBE on the most complex benchmark set of \Regel's paper~\cite{Regel20}. This set corresponds to 122 benchmarks that differ from our benchmarks in the following. First, they do not target the form validation domain. Second, they have much fewer examples (3.75 valid and 3.25 invalid, on average) which are often not enough to completely express user intent. 

\autoref{table:number-solved-resnax} shows the number of instances solved by each approach. \Forest{} DSL only supports 95 out of 122 instances. After 1 hour, \Forest{} could only return a solution to 50 of \Regel{}'s benchmarks (41\%). In 10 other benchmarks, after 1 hour, \Forest{} had already found a solution consistent with the examples, but it was still disambiguating. Of the synthesized regexes, only 13 are correct considering the natural language description provided along with the examples (11\%). In the same amount of time, \Regel{} returned 92 regexes (75\%), of which 16 (13\%) satisfy the natural language description. Note that \Regel{} can also take as input a natural language description of the problem and their evaluation shows that for this set of benchmarks, the accuracy of \Regel{} is increased substantially when using natural language~\cite{Regel20}. However, the same behavior could also be expected if \Forest{} would be extended with this kind of information.

This comparison shows the PBE engine of \Forest{} is much better than \Regel{} for form validation benchmarks where it exploits the structure of the data. For the synthesis of regexes where there is no structure, we observe that the performance of both PBE engines is comparable.

